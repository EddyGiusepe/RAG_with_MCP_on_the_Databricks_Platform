# <h1 align="center"><font color="gree">RAG with MCP on the Databricks Platform</font></h1>

<font color="pink">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>


Este projeto foi baseado no tutorial de [Aniket Maurya]().

H√° solu√ß√µes que combinam ``RAG`` com ``MCP``, onde o agente IA utiliza o MCP para acessar de maneira estruturada a dados e servi√ßos externos que alimentar√£o o processo de recupera√ß√£o e gera√ß√£o de conte√∫do, promovendo um sistema aut√¥nomo, escal√°vel e eficiente.

![](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/06/How-to-perform-RAG-using-MCP.webp)


## <font color="red">Contextualizando</font>

### <font color="blue">MCP</font>

Model Context Protocol (MCP) √© um protocolo que permite expor o contexto e funcionalidades de modelos de linguagem (``LLMs``) como servi√ßos, facilitando a cria√ß√£o de sistemas de gera√ß√£o de texto assistida por recupera√ß√£o (``RAG``). Um ``MCP server``, como no nosso c√≥digo, oferece uma interface para um modelo de linguagem com integra√ß√£o de contexto externo (em nosso caso, documentos indexados), melhorando a qualidade e relev√¢ncia das respostas.


### <font color="blue">server.py</font></font>

O arquivo server.py cria um ``servidor MCP`` que carrega documentos de uma pasta, indexa esses documentos usando um √≠ndice vetorial (``VectorStoreIndex``) e exp√µe uma API para receber consultas e gerar respostas baseadas no modelo OpenAI (``GPT-4.1-nano``). Esse servidor √© o backend do sistema ``RAG``.

### <font color="blue">client.py</font></font>

O client.py √© um cliente simples que conecta ao servidor ``MCP via HTTP``, enviando as perguntas (queries) para o endpoint ``/predict`` e exibindo as respostas. Ele serve para interagir com o servidor de forma amig√°vel, seja manualmente ou programaticamente.



Em resumo, o servidor (``server.py``) processa e responde √†s perguntas usando contexto recuperado dos documentos, e o cliente ``(client.py)`` envia essas perguntas e mostra as respostas para o usu√°rio. O ``MCP`` facilita essa arquitetura modular e conect√°vel, especialmente para aplica√ß√µes ``RAG``, onde o modelo consulta uma base de conhecimento externa para melhorar suas respostas.


## <font color="red">Explicando o uso do LitServe</font>

### <font color="blue">O que √© LitServe?</font>

``LitServe`` √© um framework Python desenvolvido pela [Lightning AI](https://pypi.org/project/litserve/) que facilita a cria√ß√£o de APIs de **infer√™ncia de modelos de Machine Learning** de forma simples e escal√°vel. Ele gerencia automaticamente todo o ciclo de vida de uma API HTTP, permitindo que voc√™ foque apenas na l√≥gica do modelo.

**Principais caracter√≠sticas:**
- üöÄ **F√°cil de usar**: Basta herdar a classe base ``LitAPI`` e implementar ``4`` m√©todos
- ‚ö° **Perform√°tico**: Otimizado para infer√™ncia de ML
- üîÑ **Gerenciamento autom√°tico**: Cuida de requisi√ß√µes HTTP, serializa√ß√£o, etc
- üì¶ **Integra√ß√£o com MCP**: Suporte nativo para Model Context Protocol


### <font color="blue">Por que LitServe N√ÉO √© como uma classe normal?</font>

A confus√£o √© natural! Em uma classe normal Python, **VOC√ä** controla quando os m√©todos s√£o executados. No LitServe, o **FRAMEWORK** controla o ciclo de vida e chama os m√©todos automaticamente baseado em eventos.

#### **Compara√ß√£o: Classe Normal vs LitAPI**

**üìå CLASSE NORMAL**

```python
class MinhaCalculadora:
    def __init__(self):
        # Voc√™ chama ao criar: calc = MinhaCalculadora()
        print("Construtor executado!")
        self.valor = 0
    
    def somar(self, x):
        # VOC√ä chama explicitamente: calc.somar(5)
        self.valor += x
        return self.valor

# VOC√ä controla tudo:
calc = MinhaCalculadora()  # __init__ executado AQUI
resultado = calc.somar(5)  # VOC√ä chama o m√©todo quando quiser
print(resultado)           # 5
```

**üìå LITAPI (padr√£o de Framework):**

```python
class DocumentChatAPI(ls.LitAPI):
    def setup(self, device):
        # Similar ao __init__, mas chamado pelo LitServe
        print("Setup executado pelo framework!")
        self.query_engine = "engine inicializado"
    
    def decode_request(self, request):
        # LitServe chama quando recebe requisi√ß√£o HTTP
        return request.query
    
    def predict(self, query):
        # LitServe chama para processar a query
        return "resposta processada"
    
    def encode_response(self, output):
        # LitServe chama para formatar a resposta
        return {"output": output}

# VOC√ä N√ÉO controla diretamente:
api = DocumentChatAPI()      # setup N√ÉO √© chamado ainda!
server = ls.LitServer(api)   # Apenas configura o servidor
server.run(port=8000)        # AQUI o LitServe assume o controle:
                             # 1. Chama setup() uma vez
                             # 2. Aguarda requisi√ß√µes HTTP
                             # 3. Para cada requisi√ß√£o, chama:
                             #    decode_request ‚Üí predict ‚Üí encode_response
```

**üîë Diferen√ßa chave:** No LitAPI voc√™ **define o comportamento** (o "como fazer"), mas o **framework decide quando executar** (o "quando fazer").

### <font color="blue">Os 4 M√©todos do LitAPI - Explica√ß√£o Detalhada</font>

O ``LitAPI`` funciona com um padr√£o de **4 m√©todos** que voc√™ deve implementar. Cada um tem um prop√≥sito espec√≠fico no ciclo de vida da API:


#### **1Ô∏è‚É£ `setup(self, device)` - O "Preparador"**

**üìù O que √©:**
- Similar ao `__init__` de uma classe, mas com timing diferente
- √â chamado **UMA √öNICA VEZ** quando o servidor inicia (em `server.run()`)
- Serve para carregar recursos pesados que voc√™ N√ÉO quer recarregar a cada requisi√ß√£o

**‚è∞ Quando √© chamado:**
- Automaticamente pelo ``LitServe`` quando `server.run(port=8000)` √© executado
- Antes de aceitar qualquer requisi√ß√£o HTTP

**üéØ Para que serve:**
- Carregar modelos de ``ML`` (ex: ``GPT``, ``BERT``)
- Inicializar conex√µes com bancos de dados
- Carregar e indexar documentos (como no meu caso com o ``LlamaIndex``)
- Configurar recursos que ser√£o reutilizados

**üí° Exemplo no nosso c√≥digo:**

```python
def setup(self, device):
    # 1. Configurar o modelo LLM (OpenAI GPT-4.1-nano)
    Settings.llm = OpenAI(
        api_key=OPENAI_API_KEY,
        temperature=0.1,
        model=model,
    )
    
    # 2. Carregar TODOS os documentos da pasta data/
    documents = SimpleDirectoryReader(
        "/caminho/para/data"
    ).load_data()
    
    # 3. Criar √≠ndice vetorial (embeddings dos documentos)
    index = VectorStoreIndex.from_documents(documents)
    
    # 4. Inicializar query engine (usado no predict)
    self.query_engine = index.as_query_engine()
    
    # Tudo isso √© feito 1 VEZ no in√≠cio!
    # Requisi√ß√µes subsequentes usam esses recursos j√° carregados
```

**‚ö†Ô∏è Por que n√£o usar `__init__`?**
- O ``LitServe`` precisa controlar quando inicializar (pode ser em workers diferentes)
- Permite passar informa√ß√µes de contexto (ex: ``device = "cpu"`` ou ``"cuda"``)
- Facilita reinicializa√ß√£o sem recriar o objeto


#### **2Ô∏è‚É£ `decode_request(self, request)` - O "Tradutor de Entrada"**

**üìù O que √©:**
- Recebe a requisi√ß√£o HTTP bruta e extrai os dados relevantes
- Chamado **TODA VEZ** que uma requisi√ß√£o HTTP chega
- √â o **PRIMEIRO** passo do processamento

**‚è∞ Quando √© chamado:**
- Automaticamente quando o cliente faz: `POST http://localhost:8000/predict`
- Antes do m√©todo `predict()`

**üéØ Para que serve:**
- Extrair dados do ``JSON`` da requisi√ß√£o
- Validar formato dos dados (feito pelo ``Pydantic``)
- Transformar dados HTTP em formato que o modelo entende

**üí° Exemplo no nosso c√≥digo:**

```python
def decode_request(self, request: RequestType):
    # request = RequestType(query="Qual a experi√™ncia do Dr. Eddy?")
    # 
    # Simplesmente extrai a string da query
    return request.query  # retorna: "Qual a experi√™ncia do Dr. Eddy?"
```

**üîÑ Fluxo:**
```
Cliente envia:
POST /predict
{"query": "Qual a experi√™ncia do Dr. Eddy?"}
         ‚Üì
LitServe recebe e cria objeto:
request = RequestType(query="Qual a experi√™ncia do Dr. Eddy?")
         ‚Üì
decode_request(request) √© chamado
         ‚Üì
Retorna: "Qual a experi√™ncia do Dr. Eddy?"
         ‚Üì
Este valor √© passado para predict()
```


#### **3Ô∏è‚É£ `predict(self, query)` - O "Processador"**

**üìù O que √©:**
- Recebe os dados decodificados e faz o processamento principal
- Aqui acontece a **l√≥gica do modelo/IA**
- Chamado **TODA VEZ** ap√≥s `decode_request()`

**‚è∞ Quando √© chamado:**
- Automaticamente ap√≥s `decode_request()` retornar
- √â o **SEGUNDO** passo do processamento

**üéØ Para que serve:**
- Executar ``infer√™ncia`` do modelo de ``ML``
- Fazer consultas ao banco vetorial (``RAG``)
- Processar a l√≥gica de neg√≥cio principal

**üí° Exemplo no nosso c√≥digo:**

```python
def predict(self, query: str):
    # query = "Qual a experi√™ncia do Dr. Eddy?"
    
    # Usa o query_engine criado no setup()
    # Ele busca nos documentos indexados e gera resposta com ``GPT-4.1-nano``
    response = self.query_engine.query(query)
    
    # response √© um objeto Response do LlamaIndex contendo:
    # - response.response: texto da resposta
    # - response.source_nodes: documentos relevantes usados
    # - response.metadata: informa√ß√µes adicionais
    
    return response
```

**üîÑ Fluxo:**
```
decode_request() retornou:
"Qual a experi√™ncia do Dr. Eddy?"
         ‚Üì
predict(query) √© chamado com essa string
         ‚Üì
Query engine busca nos documentos:
- Encontra trechos relevantes sobre experi√™ncia
- Envia contexto + pergunta para GPT-4.1-nano
- GPT gera resposta contextualizada
         ‚Üì
Retorna objeto Response:
Response(
    response="O Dr. Eddy possui experi√™ncia em...",
    source_nodes=[...]
)
         ‚Üì
Este objeto √© passado para encode_response()
```


#### **4Ô∏è‚É£ `encode_response(self, output)` - O "Formatador de Sa√≠da"**

**üìù O que √©:**
- Recebe o resultado do `predict()` e formata para JSON
- Chamado **TODA VEZ** ap√≥s `predict()`
- √â o **TERCEIRO** (√∫ltimo) passo do processamento

**‚è∞ Quando √© chamado:**
- Automaticamente ap√≥s `predict()` retornar
- Antes de enviar a resposta HTTP ao cliente

**üéØ Para que serve:**
- Converter ``objetos Python`` em ``dicion√°rio JSON``
- Formatar a resposta no padr√£o esperado pelo cliente
- Adicionar metadados extras (``timestamp``, ``vers√£o``, etc)

**üí° Exemplo no nosso c√≥digo:**

```python
def encode_response(self, output) -> dict:
    # output √© o objeto Response retornado por predict()
    
    # Extrai apenas o texto da resposta
    return {"output": output.response}
    
    # Poderia retornar mais informa√ß√µes:
    # return {
    #     "output": output.response,
    #     "sources": [node.text for node in output.source_nodes],
    #     "timestamp": datetime.now().isoformat()
    # }
```

**üîÑ Fluxo:**
```
predict() retornou:
Response(response="O Dr. Eddy possui experi√™ncia em...")
         ‚Üì
encode_response(output) √© chamado
         ‚Üì
Formata para dicion√°rio JSON:
{"output": "O Dr. Eddy possui experi√™ncia em..."}
         ‚Üì
LitServe envia resposta HTTP ao cliente:
HTTP/1.1 200 OK
Content-Type: application/json

{"output": "O Dr. Eddy possui experi√™ncia em..."}
```


### <font color="blue">Fluxo Completo - Do In√≠cio ao Fim</font>

Voc√™ pode acompanhar o que acontece desde quando voc√™ executa ``uv run server.py`` at√© o cliente receber a resposta:

#### **üöÄ FASE 1: INICIALIZA√á√ÉO (acontece 1 vez)**

```bash
# Terminal 1: Iniciando o servidor
$ uv run server.py
```

```python
# O que acontece no c√≥digo:

if __name__ == "__main__":
    # Passo 1: Criar inst√¢ncia da API
    api = DocumentChatAPI(
        mcp=MCP(description="...")
    )
    # ‚ö†Ô∏è setup() N√ÉO √© chamado ainda!
    
    # Passo 2: Criar servidor
    server = ls.LitServer(api)
    
    # Passo 3: Iniciar servidor
    server.run(port=8000)
    # ‚úÖ AQUI o LitServe chama: api.setup(device="cpu")
```

**O que acontece dentro do `setup()`:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  setup(device="cpu") √© executado                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. Configurar modelo OpenAI GPT-4.1-nano       |  
‚îÇ     ‚è±Ô∏è  ~2 segundos                             
‚îÇ                                                 | 
‚îÇ  2. Carregar documentos da pasta data/          ‚îÇ
‚îÇ     L√™ todos os arquivos da pasta ``data/``     ‚îÇ
‚îÇ     ‚è±Ô∏è  ~1-3 segundos                            
‚îÇ                                                 ‚îÇ
‚îÇ  3. Criar √≠ndice vetorial (``embeddings``)      ‚îÇ
‚îÇ     üßÆ Processa cada documento                  
‚îÇ     ‚è±Ô∏è  ~5-10 segundos                         
‚îÇ                                                 ‚îÇ
‚îÇ  4. Inicializar ``query_engine``                ‚îÇ
‚îÇ     ‚úÖ Pronto para consultas                    
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Servidor PRONTO! üéâ
Aguardando requisi√ß√µes em http://localhost:8000
```

**üí° Por que fazer isso no setup?**
- Carregar modelo e indexar documentos √© **LENTO** (pode levar 10-15 segundos)
- Se fiz√©ssemos isso a cada requisi√ß√£o, seria **INVI√ÅVEL**
- Fazendo 1 vez no in√≠cio, as requisi√ß√µes s√£o **R√ÅPIDAS** (milissegundos)

---

#### **üì® FASE 2: PROCESSAMENTO DE REQUISI√á√ÉO (toda vez)**

```bash
# Terminal 2: Cliente fazendo requisi√ß√£o
$ uv run client.py
Pergunta: Qual a experi√™ncia profissional do Dr. Eddy?
```

```python
# Cliente envia:
import requests
response = requests.post(
    "http://localhost:8000/predict",
    json={"query": "Qual a experi√™ncia profissional do Dr. Eddy Giusepe?"}
)
```

**O que acontece no servidor (automaticamente pelo LitServe):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LitServe recebe requisi√ß√£o HTTP                         ‚îÇ
‚îÇ  POST /predict                                           ‚îÇ
‚îÇ  Body: {"query": "Qual a experi√™ncia..."}                |
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASSO 1: decode_request()                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Input:  RequestType(query="Qual a experi√™ncia...")      ‚îÇ
‚îÇ  A√ß√£o:   Extrai a string da query                        ‚îÇ
‚îÇ  Output: "Qual a experi√™ncia profissional do Dr. Eddy?"  ‚îÇ
‚îÇ  ‚è±Ô∏è < 1 milissegundo                                     
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASSO 2: predict()                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Input:  "Qual a experi√™ncia profissional do Dr. Eddy?"  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  A√ß√£o 1: Query engine busca documentos relevantes        ‚îÇ
‚îÇ          - Calcula embedding da pergunta                 ‚îÇ
‚îÇ          - Busca trechos similares no √≠ndice vetorial    ‚îÇ
‚îÇ          - Encontra 3-5 chunks mais relevantes           ‚îÇ
‚îÇ          ‚è±Ô∏è  ~50-100 ms                                 
‚îÇ                                                          ‚îÇ
‚îÇ  A√ß√£o 2: Envia contexto + pergunta para GPT-4.1-nano     ‚îÇ
‚îÇ          - Monta prompt com contexto recuperado          ‚îÇ
‚îÇ          - Chama API OpenAI                              ‚îÇ
‚îÇ          - GPT gera resposta contextualizada             ‚îÇ
‚îÇ          ‚è±Ô∏è  ~500-1500 ms                                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Output: Response(                                       ‚îÇ
‚îÇ            response="O Dr. Eddy possui experi√™ncia       ‚îÇ
‚îÇ                      em Data Science, com foco em..."    ‚îÇ
‚îÇ          )                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASSO 3: encode_response()                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Input:  Response(response="O Dr. Eddy possui...")       ‚îÇ
‚îÇ  A√ß√£o:   Extrai texto e formata em dicion√°rio            ‚îÇ
‚îÇ  Output: {"output": "O Dr. Eddy possui..."}              ‚îÇ
‚îÇ  ‚è±Ô∏è  < 1 milissegundo                                    
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LitServe envia resposta HTTP ao cliente                 ‚îÇ
‚îÇ  HTTP/1.1 200 OK                                         ‚îÇ
‚îÇ  {"output": "O Dr. Eddy possui experi√™ncia em..."}       |
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cliente recebe e exibe a resposta                       ‚îÇ
‚îÇ  ‚úÖ Resposta: "O Dr. Eddy possui experi√™ncia em..."     
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

TEMPO TOTAL: ~600-1700 ms por requisi√ß√£o
```

### <font color="blue">Por que esse padr√£o √© vantajoso?</font>

#### **‚úÖ Vantagens do padr√£o LitAPI:**

1. **Separa√ß√£o de preocupa√ß√µes:**
   - Voc√™ foca na l√≥gica do seu modelo de ``ML``
   - Framework cuida da infraestrutura

2. **Performance:**
   - Recursos pesados carregados 1 vez (``setup``)
   - Requisi√ß√µes subsequentes s√£o r√°pidas

3. **Escalabilidade:**
   - ``LitServe`` pode criar m√∫ltiplos workers
   - Cada worker chama ``setup()`` uma vez
   - Todos processam requisi√ß√µes em paralelo

4. **Simplicidade:**
   - Voc√™ implementa ``4`` m√©todos simples
   - Framework cuida de ``HTTP``, ``serializa√ß√£o``, ``erros``, etc

#### **‚ùå Sem o LitServe (HTTP manual):**

```python
# Voc√™ teria que fazer tudo isso manualmente:
from flask import Flask, request, jsonify

app = Flask(__name__)

# Voc√™ gerencia tudo:
query_engine = None

@app.before_first_request
def setup():
    global query_engine
    # Inicializar modelo...
    query_engine = ...

@app.route('/predict', methods=['POST'])
def predict():
    # Validar entrada manualmente
    data = request.get_json()
    if 'query' not in data:
        return jsonify({"error": "missing query"}), 400
    
    # Processar
    query = data['query']
    response = query_engine.query(query)
    
    # Formatar sa√≠da manualmente
    return jsonify({"output": response.response})

# Gerenciar erros, logging, autentica√ß√£o, etc manualmente...
app.run(port=8000)
```

**Com LitServe voc√™ foca s√≥ na l√≥gica, n√£o na ``infraestrutura``!**

---

### <font color="blue">Resumo Final - O que voc√™ precisa lembrar</font>

| Aspecto | Detalhes |
|---------|----------|
| **O que √© LitServe?** | Framework que gerencia APIs de ML automaticamente |
| **setup()** | Inicializa√ß√£o (1 vez) - carregar recursos pesados |
| **decode_request()** | Extrair dados da requisi√ß√£o HTTP (toda requisi√ß√£o) |
| **predict()** | Processar l√≥gica do modelo (toda requisi√ß√£o) |
| **encode_response()** | Formatar resposta em JSON (toda requisi√ß√£o) |
| **Quem chama?** | O FRAMEWORK chama automaticamente, n√£o voc√™ |
| **Por que usar?** | Simplicidade + Performance + Escalabilidade |

**üîë Chave para entender:**
```
Classe normal = Voc√™ manda
LitAPI = Framework manda, voc√™ obedece (implementa os m√©todos)
```


## <font color="red">Exemplo de uso do nosso RAG-MCP</font>

![](rag_mcp.jpeg)

Thank God!