# <h1 align="center"><font color="gree">RAG with MCP on the Databricks Platform</font></h1>

<font color="pink">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>


Este projeto foi baseado no tutorial de [Aniket Maurya]().

HÃ¡ soluÃ§Ãµes que combinam ``RAG`` com ``MCP``, onde o agente IA utiliza o MCP para acessar de maneira estruturada a dados e serviÃ§os externos que alimentarÃ£o o processo de recuperaÃ§Ã£o e geraÃ§Ã£o de conteÃºdo, promovendo um sistema autÃ´nomo, escalÃ¡vel e eficiente.

![](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/06/How-to-perform-RAG-using-MCP.webp)


## <font color="red">Contextualizando</font>

### <font color="blue">MCP</font>

Model Context Protocol (MCP) Ã© um protocolo que permite expor o contexto e funcionalidades de modelos de linguagem (``LLMs``) como serviÃ§os, facilitando a criaÃ§Ã£o de sistemas de geraÃ§Ã£o de texto assistida por recuperaÃ§Ã£o (``RAG``). Um ``MCP server``, como no nosso cÃ³digo, oferece uma interface para um modelo de linguagem com integraÃ§Ã£o de contexto externo (em nosso caso, documentos indexados), melhorando a qualidade e relevÃ¢ncia das respostas.


### <font color="blue">server.py</font></font>

O arquivo server.py cria um ``servidor MCP`` que carrega documentos de uma pasta, indexa esses documentos usando um Ã­ndice vetorial (``VectorStoreIndex``) e expÃµe uma API para receber consultas e gerar respostas baseadas no modelo OpenAI (``GPT-4.1-nano``). Esse servidor Ã© o backend do sistema ``RAG``.

### <font color="blue">client.py</font></font>

O client.py Ã© um cliente simples que conecta ao servidor ``MCP via HTTP``, enviando as perguntas (queries) para o endpoint ``/predict`` e exibindo as respostas. Ele serve para interagir com o servidor de forma amigÃ¡vel, seja manualmente ou programaticamente.



Em resumo, o servidor (``server.py``) processa e responde Ã s perguntas usando contexto recuperado dos documentos, e o cliente ``(client.py)`` envia essas perguntas e mostra as respostas para o usuÃ¡rio. O ``MCP`` facilita essa arquitetura modular e conectÃ¡vel, especialmente para aplicaÃ§Ãµes ``RAG``, onde o modelo consulta uma base de conhecimento externa para melhorar suas respostas.


## <font color="red">Explicando o uso do LitServe</font>

### <font color="blue">O que Ã© LitServe?</font>

``LitServe`` Ã© um framework Python desenvolvido pela [Lightning AI](https://pypi.org/project/litserve/) que facilita a criaÃ§Ã£o de APIs de **inferÃªncia de modelos de Machine Learning** de forma simples e escalÃ¡vel. Ele gerencia automaticamente todo o ciclo de vida de uma API HTTP, permitindo que vocÃª foque apenas na lÃ³gica do modelo.

**Principais caracterÃ­sticas:**
- ğŸš€ **FÃ¡cil de usar**: Basta herdar a classe base ``LitAPI`` e implementar ``4`` mÃ©todos
- âš¡ **PerformÃ¡tico**: Otimizado para inferÃªncia de ML
- ğŸ”„ **Gerenciamento automÃ¡tico**: Cuida de requisiÃ§Ãµes HTTP, serializaÃ§Ã£o, etc
- ğŸ“¦ **IntegraÃ§Ã£o com MCP**: Suporte nativo para Model Context Protocol


### <font color="blue">Por que LitServe NÃƒO Ã© como uma classe normal?</font>

A confusÃ£o Ã© natural! Em uma classe normal Python, **VOCÃŠ** controla quando os mÃ©todos sÃ£o executados. No LitServe, o **FRAMEWORK** controla o ciclo de vida e chama os mÃ©todos automaticamente baseado em eventos.

#### **ComparaÃ§Ã£o: Classe Normal vs LitAPI**

**ğŸ“Œ CLASSE NORMAL**

```python
class MinhaCalculadora:
    def __init__(self):
        # VocÃª chama ao criar: calc = MinhaCalculadora()
        print("Construtor executado!")
        self.valor = 0
    
    def somar(self, x):
        # VOCÃŠ chama explicitamente: calc.somar(5)
        self.valor += x
        return self.valor

# VOCÃŠ controla tudo:
calc = MinhaCalculadora()  # __init__ executado AQUI
resultado = calc.somar(5)  # VOCÃŠ chama o mÃ©todo quando quiser
print(resultado)           # 5
```

**ğŸ“Œ LITAPI (padrÃ£o de Framework):**

```python
class DocumentChatAPI(ls.LitAPI):
    def setup(self, device):
        # Similar ao __init__, mas chamado pelo LitServe
        print("Setup executado pelo framework!")
        self.query_engine = "engine inicializado"
    
    def decode_request(self, request):
        # LitServe chama quando recebe requisiÃ§Ã£o HTTP
        return request.query
    
    def predict(self, query):
        # LitServe chama para processar a query
        return "resposta processada"
    
    def encode_response(self, output):
        # LitServe chama para formatar a resposta
        return {"output": output}

# VOCÃŠ NÃƒO controla diretamente:
api = DocumentChatAPI()      # setup NÃƒO Ã© chamado ainda!
server = ls.LitServer(api)   # Apenas configura o servidor
server.run(port=8000)        # AQUI o LitServe assume o controle:
                             # 1. Chama setup() uma vez
                             # 2. Aguarda requisiÃ§Ãµes HTTP
                             # 3. Para cada requisiÃ§Ã£o, chama:
                             #    decode_request â†’ predict â†’ encode_response
```

**ğŸ”‘ DiferenÃ§a chave:** No LitAPI vocÃª **define o comportamento** (o "como fazer"), mas o **framework decide quando executar** (o "quando fazer").

### <font color="blue">Os 4 MÃ©todos do LitAPI - ExplicaÃ§Ã£o Detalhada</font>

O ``LitAPI`` funciona com um padrÃ£o de **4 mÃ©todos** que vocÃª deve implementar. Cada um tem um propÃ³sito especÃ­fico no ciclo de vida da API:


#### **1ï¸âƒ£ `setup(self, device)` - O "Preparador"**

**ğŸ“ O que Ã©:**
- Similar ao `__init__` de uma classe, mas com timing diferente
- Ã‰ chamado **UMA ÃšNICA VEZ** quando o servidor inicia (em `server.run()`)
- Serve para carregar recursos pesados que vocÃª NÃƒO quer recarregar a cada requisiÃ§Ã£o

**â° Quando Ã© chamado:**
- Automaticamente pelo ``LitServe`` quando `server.run(port=8000)` Ã© executado
- Antes de aceitar qualquer requisiÃ§Ã£o HTTP

**ğŸ¯ Para que serve:**
- Carregar modelos de ``ML`` (ex: ``GPT``, ``BERT``)
- Inicializar conexÃµes com bancos de dados
- Carregar e indexar documentos (como no meu caso com o ``LlamaIndex``)
- Configurar recursos que serÃ£o reutilizados

**ğŸ’¡ Exemplo no nosso cÃ³digo:**

```python
def setup(self, device):
    # 1. Configurar o modelo LLM (OpenAI GPT-4.1-nano)
    Settings.llm = OpenAI(
        api_key=OPENAI_API_KEY,
        temperature=0.1,
        model=model,
    )
    
    # 2. Carregar TODOS os documentos da pasta data/
    documents = SimpleDirectoryReader(
        "/caminho/para/data"
    ).load_data()
    
    # 3. Criar Ã­ndice vetorial (embeddings dos documentos)
    index = VectorStoreIndex.from_documents(documents)
    
    # 4. Inicializar query engine (usado no predict)
    self.query_engine = index.as_query_engine()
    
    # Tudo isso Ã© feito 1 VEZ no inÃ­cio!
    # RequisiÃ§Ãµes subsequentes usam esses recursos jÃ¡ carregados
```

**âš ï¸ Por que nÃ£o usar `__init__`?**
- O ``LitServe`` precisa controlar quando inicializar (pode ser em workers diferentes)
- Permite passar informaÃ§Ãµes de contexto (ex: ``device = "cpu"`` ou ``"cuda"``)
- Facilita reinicializaÃ§Ã£o sem recriar o objeto


#### **2ï¸âƒ£ `decode_request(self, request)` - O "Tradutor de Entrada"**

**ğŸ“ O que Ã©:**
- Recebe a requisiÃ§Ã£o HTTP bruta e extrai os dados relevantes
- Chamado **TODA VEZ** que uma requisiÃ§Ã£o HTTP chega
- Ã‰ o **PRIMEIRO** passo do processamento

**â° Quando Ã© chamado:**
- Automaticamente quando o cliente faz: `POST http://localhost:8000/predict`
- Antes do mÃ©todo `predict()`

**ğŸ¯ Para que serve:**
- Extrair dados do ``JSON`` da requisiÃ§Ã£o
- Validar formato dos dados (feito pelo ``Pydantic``)
- Transformar dados HTTP em formato que o modelo entende

**ğŸ’¡ Exemplo no nosso cÃ³digo:**

```python
def decode_request(self, request: RequestType):
    # request = RequestType(query="Qual a experiÃªncia do Dr. Eddy?")
    # 
    # Simplesmente extrai a string da query
    return request.query  # retorna: "Qual a experiÃªncia do Dr. Eddy?"
```

**ğŸ”„ Fluxo:**
```
Cliente envia:
POST /predict
{"query": "Qual a experiÃªncia do Dr. Eddy?"}
         â†“
LitServe recebe e cria objeto:
request = RequestType(query="Qual a experiÃªncia do Dr. Eddy?")
         â†“
decode_request(request) Ã© chamado
         â†“
Retorna: "Qual a experiÃªncia do Dr. Eddy?"
         â†“
Este valor Ã© passado para predict()
```


#### **3ï¸âƒ£ `predict(self, query)` - O "Processador"**

**ğŸ“ O que Ã©:**
- Recebe os dados decodificados e faz o processamento principal
- Aqui acontece a **lÃ³gica do modelo/IA**
- Chamado **TODA VEZ** apÃ³s `decode_request()`

**â° Quando Ã© chamado:**
- Automaticamente apÃ³s `decode_request()` retornar
- Ã‰ o **SEGUNDO** passo do processamento

**ğŸ¯ Para que serve:**
- Executar ``inferÃªncia`` do modelo de ``ML``
- Fazer consultas ao banco vetorial (``RAG``)
- Processar a lÃ³gica de negÃ³cio principal

**ğŸ’¡ Exemplo no nosso cÃ³digo:**

```python
def predict(self, query: str):
    # query = "Qual a experiÃªncia do Dr. Eddy?"
    
    # Usa o query_engine criado no setup()
    # Ele busca nos documentos indexados e gera resposta com ``GPT-4.1-nano``
    response = self.query_engine.query(query)
    
    # response Ã© um objeto Response do LlamaIndex contendo:
    # - response.response: texto da resposta
    # - response.source_nodes: documentos relevantes usados
    # - response.metadata: informaÃ§Ãµes adicionais
    
    return response
```

**ğŸ”„ Fluxo:**
```
decode_request() retornou:
"Qual a experiÃªncia do Dr. Eddy?"
         â†“
predict(query) Ã© chamado com essa string
         â†“
Query engine busca nos documentos:
- Encontra trechos relevantes sobre experiÃªncia
- Envia contexto + pergunta para GPT-4.1-nano
- GPT gera resposta contextualizada
         â†“
Retorna objeto Response:
Response(
    response="O Dr. Eddy possui experiÃªncia em...",
    source_nodes=[...]
)
         â†“
Este objeto Ã© passado para encode_response()
```


#### **4ï¸âƒ£ `encode_response(self, output)` - O "Formatador de SaÃ­da"**

**ğŸ“ O que Ã©:**
- Recebe o resultado do `predict()` e formata para JSON
- Chamado **TODA VEZ** apÃ³s `predict()`
- Ã‰ o **TERCEIRO** (Ãºltimo) passo do processamento

**â° Quando Ã© chamado:**
- Automaticamente apÃ³s `predict()` retornar
- Antes de enviar a resposta HTTP ao cliente

**ğŸ¯ Para que serve:**
- Converter ``objetos Python`` em ``dicionÃ¡rio JSON``
- Formatar a resposta no padrÃ£o esperado pelo cliente
- Adicionar metadados extras (``timestamp``, ``versÃ£o``, etc)

**ğŸ’¡ Exemplo no nosso cÃ³digo:**

```python
def encode_response(self, output) -> dict:
    # output Ã© o objeto Response retornado por predict()
    
    # Extrai apenas o texto da resposta
    return {"output": output.response}
    
    # Poderia retornar mais informaÃ§Ãµes:
    # return {
    #     "output": output.response,
    #     "sources": [node.text for node in output.source_nodes],
    #     "timestamp": datetime.now().isoformat()
    # }
```

**ğŸ”„ Fluxo:**
```
predict() retornou:
Response(response="O Dr. Eddy possui experiÃªncia em...")
         â†“
encode_response(output) Ã© chamado
         â†“
Formata para dicionÃ¡rio JSON:
{"output": "O Dr. Eddy possui experiÃªncia em..."}
         â†“
LitServe envia resposta HTTP ao cliente:
HTTP/1.1 200 OK
Content-Type: application/json

{"output": "O Dr. Eddy possui experiÃªncia em..."}
```


### <font color="blue">Fluxo Completo - Do InÃ­cio ao Fim</font>

VocÃª pode acompanhar o que acontece desde quando vocÃª executa ``uv run server.py`` atÃ© o cliente receber a resposta:

#### **ğŸš€ FASE 1: INICIALIZAÃ‡ÃƒO (acontece 1 vez)**

```bash
# Terminal 1: Iniciando o servidor
$ uv run server.py
```

```python
# O que acontece no cÃ³digo:

if __name__ == "__main__":
    # Passo 1: Criar instÃ¢ncia da API
    api = DocumentChatAPI(
        mcp=MCP(description="...")
    )
    # âš ï¸ setup() NÃƒO Ã© chamado ainda!
    
    # Passo 2: Criar servidor
    server = ls.LitServer(api)
    
    # Passo 3: Iniciar servidor
    server.run(port=8000)
    # âœ… AQUI o LitServe chama: api.setup(device="cpu")
```

**O que acontece dentro do `setup()`:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  setup(device="cpu") Ã© executado                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Configurar modelo OpenAI GPT-4.1-nano       |  
â”‚     â±ï¸  ~2 segundos                             
â”‚                                                 | 
â”‚  2. Carregar documentos da pasta data/          â”‚
â”‚     LÃª todos os arquivos da pasta ``data/``     â”‚
â”‚     â±ï¸  ~1-3 segundos                            
â”‚                                                 â”‚
â”‚  3. Criar Ã­ndice vetorial (``embeddings``)      â”‚
â”‚     ğŸ§® Processa cada documento                  
â”‚     â±ï¸  ~5-10 segundos                         
â”‚                                                 â”‚
â”‚  4. Inicializar ``query_engine``                â”‚
â”‚     âœ… Pronto para consultas                    
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Servidor PRONTO! ğŸ‰
Aguardando requisiÃ§Ãµes em http://localhost:8000
```

**ğŸ’¡ Por que fazer isso no setup?**
- Carregar modelo e indexar documentos Ã© **LENTO** (pode levar 10-15 segundos)
- Se fizÃ©ssemos isso a cada requisiÃ§Ã£o, seria **INVIÃVEL**
- Fazendo 1 vez no inÃ­cio, as requisiÃ§Ãµes sÃ£o **RÃPIDAS** (milissegundos)

---

#### **ğŸ“¨ FASE 2: PROCESSAMENTO DE REQUISIÃ‡ÃƒO (toda vez)**

```bash
# Terminal 2: Cliente fazendo requisiÃ§Ã£o
$ uv run client.py
Pergunta: Qual a experiÃªncia profissional do Dr. Eddy?
```

```python
# Cliente envia:
import requests
response = requests.post(
    "http://localhost:8000/predict",
    json={"query": "Qual a experiÃªncia profissional do Dr. Eddy Giusepe?"}
)
```

**O que acontece no servidor (automaticamente pelo LitServe):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LitServe recebe requisiÃ§Ã£o HTTP                         â”‚
â”‚  POST /predict                                           â”‚
â”‚  Body: {"query": "Qual a experiÃªncia..."}                |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASSO 1: decode_request()                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input:  RequestType(query="Qual a experiÃªncia...")      â”‚
â”‚  AÃ§Ã£o:   Extrai a string da query                        â”‚
â”‚  Output: "Qual a experiÃªncia profissional do Dr. Eddy?"  â”‚
â”‚  â±ï¸ < 1 milissegundo                                     
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASSO 2: predict()                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input:  "Qual a experiÃªncia profissional do Dr. Eddy?"  â”‚
â”‚                                                          â”‚
â”‚  AÃ§Ã£o 1: Query engine busca documentos relevantes        â”‚
â”‚          - Calcula embedding da pergunta                 â”‚
â”‚          - Busca trechos similares no Ã­ndice vetorial    â”‚
â”‚          - Encontra 3-5 chunks mais relevantes           â”‚
â”‚          â±ï¸  ~50-100 ms                                 
â”‚                                                          â”‚
â”‚  AÃ§Ã£o 2: Envia contexto + pergunta para GPT-4.1-nano     â”‚
â”‚          - Monta prompt com contexto recuperado          â”‚
â”‚          - Chama API OpenAI                              â”‚
â”‚          - GPT gera resposta contextualizada             â”‚
â”‚          â±ï¸  ~500-1500 ms                                â”‚
â”‚                                                          â”‚
â”‚  Output: Response(                                       â”‚
â”‚            response="O Dr. Eddy possui experiÃªncia       â”‚
â”‚                      em Data Science, com foco em..."    â”‚
â”‚          )                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASSO 3: encode_response()                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input:  Response(response="O Dr. Eddy possui...")       â”‚
â”‚  AÃ§Ã£o:   Extrai texto e formata em dicionÃ¡rio            â”‚
â”‚  Output: {"output": "O Dr. Eddy possui..."}              â”‚
â”‚  â±ï¸  < 1 milissegundo                                    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LitServe envia resposta HTTP ao cliente                 â”‚
â”‚  HTTP/1.1 200 OK                                         â”‚
â”‚  {"output": "O Dr. Eddy possui experiÃªncia em..."}       |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cliente recebe e exibe a resposta                       â”‚
â”‚  âœ… Resposta: "O Dr. Eddy possui experiÃªncia em..."     
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TEMPO TOTAL: ~600-1700 ms por requisiÃ§Ã£o
```

### <font color="blue">Por que esse padrÃ£o Ã© vantajoso?</font>

#### **âœ… Vantagens do padrÃ£o LitAPI:**

1. **SeparaÃ§Ã£o de preocupaÃ§Ãµes:**
   - VocÃª foca na lÃ³gica do seu modelo de ``ML``
   - Framework cuida da infraestrutura

2. **Performance:**
   - Recursos pesados carregados 1 vez (``setup``)
   - RequisiÃ§Ãµes subsequentes sÃ£o rÃ¡pidas

3. **Escalabilidade:**
   - ``LitServe`` pode criar mÃºltiplos workers
   - Cada worker chama ``setup()`` uma vez
   - Todos processam requisiÃ§Ãµes em paralelo

4. **Simplicidade:**
   - VocÃª implementa ``4`` mÃ©todos simples
   - Framework cuida de ``HTTP``, ``serializaÃ§Ã£o``, ``erros``, etc

#### **âŒ Sem o LitServe (HTTP manual):**

```python
# VocÃª teria que fazer tudo isso manualmente:
from flask import Flask, request, jsonify

app = Flask(__name__)

# VocÃª gerencia tudo:
query_engine = None

@app.before_first_request
def setup():
    global query_engine
    # Inicializar modelo...
    query_engine = ...

@app.route('/predict', methods=['POST'])
def predict():
    # Validar entrada manualmente
    data = request.get_json()
    if 'query' not in data:
        return jsonify({"error": "missing query"}), 400
    
    # Processar
    query = data['query']
    response = query_engine.query(query)
    
    # Formatar saÃ­da manualmente
    return jsonify({"output": response.response})

# Gerenciar erros, logging, autenticaÃ§Ã£o, etc manualmente...
app.run(port=8000)
```

**Com LitServe vocÃª foca sÃ³ na lÃ³gica, nÃ£o na ``infraestrutura``!**

---

### <font color="blue">Resumo Final - O que vocÃª precisa lembrar</font>

| Aspecto | Detalhes |
|---------|----------|
| **O que Ã© LitServe?** | Framework que gerencia APIs de ML automaticamente |
| **setup()** | InicializaÃ§Ã£o (1 vez) - carregar recursos pesados |
| **decode_request()** | Extrair dados da requisiÃ§Ã£o HTTP (toda requisiÃ§Ã£o) |
| **predict()** | Processar lÃ³gica do modelo (toda requisiÃ§Ã£o) |
| **encode_response()** | Formatar resposta em JSON (toda requisiÃ§Ã£o) |
| **Quem chama?** | O FRAMEWORK chama automaticamente, nÃ£o vocÃª |
| **Por que usar?** | Simplicidade + Performance + Escalabilidade |

**ğŸ”‘ Chave para entender:**
```
Classe normal = VocÃª manda
LitAPI = Framework manda, vocÃª obedece (implementa os mÃ©todos)
```


## <font color="red">Exemplo de uso do nosso RAG-MCP</font>

![](rag_mcp.jpeg)

Thank God!